<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Language Models are Learnable from Scratch in Low-Resource Settings - Msingi AI</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
    <link href="https://unpkg.com/@tailwindcss/typography@0.4.1/dist/typography.min.css" rel="stylesheet">
    <script src="../../js/social-share.js"></script>
</head>
<body class="bg-gray-50">
    <!-- Navigation -->
    <nav class="bg-white shadow-lg fixed w-full z-10">
        <div class="max-w-7xl mx-auto px-4">
            <div class="flex justify-between h-16">
                <div class="flex items-center">
                    <a href="../.." class="hover:opacity-90 transition-opacity duration-300">
                        <img src="../../assets/msingiailogo.jpeg" alt="Msingi AI - Building AI That Understands Africa" class="h-12 w-auto" />
                    </a>
                </div>
                <div class="hidden md:flex items-center space-x-8">
                    <a href="../.." class="text-gray-700 hover:text-indigo-600">Home</a>
                    <a href="../../research.html" class="text-gray-700 hover:text-indigo-600">Research</a>
                    <a href="../../community.html" class="text-gray-700 hover:text-indigo-600">Community</a>
                    <a href="../../blog.html" class="text-gray-700 hover:text-indigo-600">Blog</a>
                </div>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <div class="pt-24">
        <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
            <!-- Title Section -->
            <div class="bg-white rounded-lg shadow-lg overflow-hidden mb-8">
                <div class="p-8">
                    <h1 class="text-4xl font-bold text-gray-900 mb-4">Language Models are Learnable from Scratch in Low-Resource Settings</h1>
                    <div class="flex items-center text-gray-600 mb-8">
                        <div class="flex items-center">
                            <img src="../../images/authors/msingiailogo.jpeg" 
                                alt="Msingi AI Research Team" 
                                class="w-10 h-10 rounded-full object-cover mr-3">
                            <div>
                                <span class="font-medium text-gray-900 block">Msingi AI Research Team</span>
                                <time datetime="April 18, 2025" class="text-sm">April 18, 2025</time>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Content Section -->
            <div class="bg-white rounded-lg shadow-lg overflow-hidden">
                <div class="p-8">
                    <div class="prose prose-indigo max-w-none">
                        <h2>Overview</h2>
<p>The majority of natural language processing (NLP) research today revolves around <strong>multilingual models</strong> that leverage massive pre-trained architectures like mBERT or GPT-3. The idea is simple: <em>fine-tune a large model on your language of interest</em>. But what happens when you strip away these enormous pre-trained models and build something from the ground up? </p>
<p>In this post, we share our journey of building <strong>Msingi1</strong>, a <strong>Swahili-first language model</strong> trained entirely from scratch. With <strong>no reliance on multilingual corpora</strong>, no pretraining on English data, and no <strong>gigantic parameters</strong>, Msingi1 represents a small but significant step towards understanding how language models can be created for <strong>low-resource languages</strong>.</p>
<p>Our primary goal was simple: </p>
<blockquote>
<p><strong>Can a tiny GPT-style model, trained on just Swahili data, learn to generate meaningful text?</strong></p>
</blockquote>
<p>After training Msingi1 for <strong>4 hours on a T4 GPU</strong>, here’s what we learned.</p>
<h2>1. Why Train From Scratch?</h2>
<p>The current landscape of NLP is dominated by <strong>fine-tuning pre-trained models</strong>, like mBERT or GPT-2. These models are pre-trained on vast multilingual corpora that include English, Chinese, French, and hundreds of other languages. While this approach works for many languages, especially high-resource ones, it comes with several problems for low-resource languages:</p>
<ul>
<li><strong>Bias in Tokenization</strong>: Multilingual tokenizers often prioritize English-centric tokenization strategies that are not ideal for languages like Swahili, which have unique morphological structures.</li>
<li><strong>Mistranslation Issues</strong>: Pre-trained multilingual models may have poor performance on languages like Swahili, as they are trained on noisy, often non-native translations, and don’t fully understand the language’s cultural or contextual meaning.</li>
<li><strong>Lack of True Language Understanding</strong>: These models “know” a language but don’t <em>think</em> in it. They’ve been taught English as a base and only partially incorporate the logic and structure of languages like Swahili.</li>
</ul>
<p>By training a model <strong>from scratch</strong>, we allow the system to learn directly from the raw Swahili text — no interference from English, no pre-baked assumptions. This approach also allows us to explore <strong>how small models can learn without the crutch of pretraining</strong> on a language the model didn’t natively understand.</p>
<h2>2. Model Architecture</h2>
<p>We opted for a <strong>GPT-style transformer</strong> with 6 layers and a relatively small number of parameters. This choice ensured that we could train the model with limited computational resources while still experimenting with a model architecture that reflects those used by much larger models like GPT-3.</p>
<h3>Architecture Details:</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Layers</strong></td>
<td>6</td>
</tr>
<tr>
<td><strong>Hidden size</strong></td>
<td>384</td>
</tr>
<tr>
<td><strong>Attention heads</strong></td>
<td>6</td>
</tr>
<tr>
<td><strong>Parameters</strong></td>
<td>~85M</td>
</tr>
<tr>
<td><strong>Sequence length</strong></td>
<td>512 tokens</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>Decoder-only</td>
</tr>
</tbody>
</table>
<p>At ~85 million parameters, this model is small by modern standards — significantly smaller than GPT-3 or even GPT-2. Yet it still contains enough complexity to model the intricate syntax and semantics of Swahili, a <strong>Bantu language</strong> with complex noun-class systems and verb morphology.</p>
<p>This decision to go small was intentional. We didn’t want to take shortcuts or rely on enormous compute power. We wanted to explore <strong>what a compact model can achieve</strong> and examine where its boundaries lie.</p>
<h2>3. Dataset and Preprocessing</h2>
<p>The dataset we used for Msingi1 is diverse, spanning across several genres of Swahili text. We collected approximately <strong>45 million tokens</strong> from the following sources:</p>
<ul>
<li><strong>News articles</strong> from local publications</li>
<li><strong>Public domain books</strong>, particularly Swahili literature</li>
<li><strong>Religious texts</strong> (the Quran, Bible)</li>
<li><strong>Government speeches and proceedings</strong></li>
<li><strong>Web scraping</strong> from Swahili pages</li>
</ul>
<p>This diverse corpus ensured that the model would be exposed to a wide range of Swahili, from formal language in news reports to more colloquial expressions in online discussions.</p>
<p>For tokenization, we decided to use a <strong>Unigram tokenizer</strong> with a vocabulary size of 32,000. We chose Unigram because it strikes a balance between efficiency and handling rare words — important for Swahili’s rich morphology.</p>
<p>The <strong>SentencePiece</strong> tokenizer allowed us to manage the nuances of Swahili’s morphology, which can often result in long compound words. Unlike English, Swahili often uses affixes, prefixes, and suffixes that change the meaning of the root word significantly, so choosing the right tokenization method was critical.</p>
<h2>4. Training Setup</h2>
<p>Training Msingi1 was a real test of patience. We used a <strong>T4 GPU</strong> with a <strong>batch size of 8</strong>, training for <strong>4 hours</strong>. Given the size of the model and the hardware constraints, this was an aggressive but necessary choice to understand how quickly the model could converge on meaningful patterns.</p>
<h3>Training Details:</h3>
<ul>
<li><strong>Optimizer</strong>: AdamW</li>
<li><strong>Learning rate</strong>: Linearly warmed up, then decayed</li>
<li><strong>Weight decay</strong>: 0.01</li>
<li><strong>Batch size</strong>: 8</li>
<li><strong>Effective batch size</strong>: 8</li>
<li><strong>Sequence length</strong>: 512 tokens</li>
</ul>
<p>Despite the limited batch size, the model showed stable training dynamics. Loss decreased consistently without any signs of divergence or overfitting. This is a clear indication that even smaller models, when trained from scratch on the right data, can <strong>learn meaningful representations</strong>.</p>
<p>The relatively small batch size was manageable on the T4, but there were certainly limitations. Longer training durations and larger batches could likely improve the model’s final performance, but this small-scale experiment was designed to test the core viability of training such a model.</p>
<h2>5. Generation Results</h2>
<p>We experimented with <strong>text generation</strong> to evaluate how well Msingi1 can handle the Swahili language. The most interesting results came from the <strong>sampling parameters</strong> we used. We varied the temperature, top-k, and repetition penalties to see how the model’s outputs would change.</p>
<h3>Sample Output:</h3>
<p><strong>Prompt</strong>:<br />
<code>Habari ya leo ni</code></p>
<p><strong>Generated Output</strong>:<br />
<code>Habari ya leo ni mbili sheria sheria sana eneo tena jeshi bila...</code></p>
<p>🧠 <strong>Observation</strong>: The model is able to generate valid Swahili syntax, although repetition of words like "sheria" (law) occurs frequently. This type of behavior is common in smaller models, where word repetition may occur due to the model’s limited context window.</p>
<p><strong>Prompt</strong>:<br />
<code>Rais wa Jamhuri ya Muungano wa</code></p>
<p><strong>Generated Output</strong>:<br />
<code>Rais wa Jamhuri ya Muungano wa Tanzania ametangaza kuwa uchaguzi mkuu utafanyika mwezi ujao.</code></p>
<p>✅ <strong>Observation</strong>: The model generates more coherent output when the prompt is formal and contextually rich. The sentence is both grammatically correct and contextually relevant, demonstrating that the model has learned how to handle formal Swahili.</p>
<p>These experiments showed that Msingi1 is capable of generating meaningful Swahili text, though it still needs improvements in fluency and repetition control.</p>
<h2>6. Lessons Learned</h2>
<p>From our journey with Msingi1, we uncovered several key lessons that will inform our next steps:</p>
<ul>
<li><strong>Swahili is learnable</strong> from scratch with limited data and small models. We were able to capture significant syntactic and semantic structure, even in a model with just 85M parameters.</li>
<li><strong>Tokenization is critical</strong>. We saw that using a Unigram tokenizer tailored to Swahili's morphology helped prevent fragmentation and reduced repetition, which is crucial in low-resource languages.</li>
<li><strong>Repetition penalty is essential</strong>. Without it, smaller models tend to repeat phrases and words, particularly when generating longer texts. This behavior is amplified in Swahili, where verbs and nouns often repeat in long compound forms.</li>
<li><strong>Context and domain diversity matter</strong>. Exposure to a wide range of Swahili sources helped the model handle both formal and informal text with more consistency.</li>
</ul>
<h2>7. Why This Matters</h2>
<p>Training a model from scratch for Swahili demonstrates that <strong>low-resource languages</strong> don’t need to rely on multilingual models or massive datasets to develop functional, meaningful models. </p>
<p>This experiment highlights the potential for building <strong>language-specific models</strong> for African languages and other low-resource languages. It's not about creating the biggest model possible — it's about building a foundation that reflects the structure and nuances of the language itself.</p>
<h2>8. Next Steps</h2>
<p>We plan to:</p>
<ul>
<li>Compare <strong>Msingi1</strong> with a distilled GPT-2 model trained on the same dataset.</li>
<li>Fine-tune <strong>Msingi1</strong> on <strong>domain-specific corpora</strong>, such as medical and educational content, to improve its utility in real-world applications.</li>
<li>Open-source the model’s <strong>weights</strong> and <strong>tokenizer</strong> for others to use and contribute to.</li>
<li>Explore <strong>alignment</strong> techniques such as reinforcement learning from human feedback (RLHF) for better usability in Swahili-centric applications.</li>
</ul>
<h2>Final Thought</h2>
<p>Language models are not just about scaling up. They’re about understanding, interpreting, and reflecting the languages we work with. <strong>Msingi1</strong> is a small but meaningful step toward creating <strong>equitable NLP tools</strong> that respect language and culture, from the ground up.</p>
<p><strong>Follow @MsingiAI</strong> for future updates as we continue to refine and expand this exciting project.</p>
                    </div>
                    <div class="mt-8 pt-8 border-t border-gray-200 flex items-center justify-between">
                        <a href="../../blog.html" class="text-indigo-600 hover:text-indigo-700">
                            ← Back to Blog
                        </a>
                        <div class="flex items-center space-x-4">
                            <button onclick="shareOnLinkedIn()" class="text-blue-600 hover:text-blue-700 flex items-center">
                                <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 20 20">
                                    <path d="M15.8 0H4.2C1.88 0 0 1.88 0 4.2v11.6C0 18.12 1.88 20 4.2 20h11.6c2.32 0 4.2-1.88 4.2-4.2V4.2C20 1.88 18.12 0 15.8 0zM6.5 17H3.8V7.5h2.7V17zM5.15 6.2c-.87 0-1.57-.7-1.57-1.57s.7-1.57 1.57-1.57 1.57.7 1.57 1.57-.7 1.57-1.57 1.57zm11.85 10.8h-2.7v-4.63c0-1.1-.02-2.52-1.53-2.52-1.53 0-1.77 1.2-1.77 2.44V17h-2.7V7.5h2.58v1.18h.04c.4-.75 1.37-1.53 2.82-1.53 3.02 0 3.57 1.98 3.57 4.56V17z"/>
                                </svg>
                                Share on LinkedIn
                            </button>
                            <button onclick="shareOnX()" class="text-gray-800 hover:text-gray-900 flex items-center">
                                <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 24 24">
                                    <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                                </svg>
                                Share on X
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>
</html>